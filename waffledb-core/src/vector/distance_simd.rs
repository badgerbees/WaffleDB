/// SIMD-accelerated distance computations using AVX2/AVX512.
/// 
/// This module provides vectorized distance calculations for L2, Cosine, and Inner Product.
/// Falls back to scalar implementations on unsupported platforms.

/// Compute L2 (Euclidean) distance with SIMD acceleration (AVX2).
/// For vectors larger than 256 dims, processes in 8-element chunks.
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
pub fn l2_distance_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len(), "Vectors must have same dimension");
    
    unsafe {
        let mut sum_vec = _mm256_setzero_ps();
        let mut i = 0;
        
        // Process 8 floats at a time (256 bits / 32 bits per float)
        while i + 8 <= a.len() {
            let a_vec = _mm256_loadu_ps(&a[i]);
            let b_vec = _mm256_loadu_ps(&b[i]);
            
            // Compute (a - b)
            let diff = _mm256_sub_ps(a_vec, b_vec);
            // Compute (a - b)^2
            let sq = _mm256_mul_ps(diff, diff);
            // Accumulate
            sum_vec = _mm256_add_ps(sum_vec, sq);
            
            i += 8;
        }
        
        // Horizontal sum of 8 floats
        let mut sum = sum_vector_avx2(sum_vec);
        
        // Handle remaining elements
        while i < a.len() {
            let diff = a[i] - b[i];
            sum += diff * diff;
            i += 1;
        }
        
        sum.sqrt()
    }
}

/// Fallback scalar L2 distance
#[cfg(not(all(target_arch = "x86_64", target_feature = "avx2")))]
pub fn l2_distance_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len());
    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f32>()
        .sqrt()
}

/// Compute cosine distance with SIMD acceleration (AVX2).
/// Assumes vectors are normalized.
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
pub fn cosine_distance_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len());
    
    unsafe {
        let dot = inner_product_simd_avx2(a, b);
        1.0 - dot
    }
}

/// Fallback scalar cosine distance
#[cfg(not(all(target_arch = "x86_64", target_feature = "avx2")))]
pub fn cosine_distance_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len());
    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    1.0 - dot
}

/// Compute inner product with SIMD acceleration (AVX2).
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
pub fn inner_product_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len());
    unsafe { inner_product_simd_avx2(a, b) }
}

/// Fallback scalar inner product
#[cfg(not(all(target_arch = "x86_64", target_feature = "avx2")))]
pub fn inner_product_simd(a: &[f32], b: &[f32]) -> f32 {
    debug_assert_eq!(a.len(), b.len());
    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()
}

/// SIMD-accelerated inner product using AVX2
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
unsafe fn inner_product_simd_avx2(a: &[f32], b: &[f32]) -> f32 {
    let mut dot_vec = _mm256_setzero_ps();
    let mut i = 0;
    
    // Process 8 floats at a time
    while i + 8 <= a.len() {
        let a_vec = _mm256_loadu_ps(&a[i]);
        let b_vec = _mm256_loadu_ps(&b[i]);
        
        // Compute a * b
        let prod = _mm256_mul_ps(a_vec, b_vec);
        // Accumulate
        dot_vec = _mm256_add_ps(dot_vec, prod);
        
        i += 8;
    }
    
    // Horizontal sum
    let mut dot = sum_vector_avx2(dot_vec);
    
    // Handle remaining elements
    while i < a.len() {
        dot += a[i] * b[i];
        i += 1;
    }
    
    dot
}

/// Horizontal sum of 8 floats in AVX2 vector
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
unsafe fn sum_vector_avx2(v: __m256) -> f32 {
    // Shuffle and add to sum across lanes
    let v = _mm256_hadd_ps(v, v);
    let v = _mm256_hadd_ps(v, v);
    
    // Extract lowest element and add high lane
    let sum_low = _mm_cvtss_f32(_mm256_castps256_ps128(v));
    let sum_high = _mm_cvtss_f32(_mm256_extractf128_ps(v, 1));
    
    sum_low + sum_high
}

/// PQ-ADC distance computation with SIMD lookup table acceleration.
/// Computes distance from full query to PQ-compressed codes using precomputed lookup table.
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
pub fn adc_distance_simd(codes: &[u8], lookup_table: &[Vec<f32>]) -> f32 {
    unsafe {
        let mut sum_vec = _mm256_setzero_ps();
        let mut i = 0;
        
        // Process 8 lookups at a time
        while i + 8 <= codes.len() && i + 8 <= lookup_table.len() {
            let mut values = [0.0f32; 8];
            for j in 0..8 {
                if (codes[i + j] as usize) < lookup_table[i + j].len() {
                    values[j] = lookup_table[i + j][codes[i + j] as usize];
                }
            }
            
            let vals_vec = _mm256_loadu_ps(&values);
            sum_vec = _mm256_add_ps(sum_vec, vals_vec);
            i += 8;
        }
        
        let mut sum = sum_vector_avx2(sum_vec);
        
        // Handle remaining elements
        while i < codes.len() && i < lookup_table.len() {
            if (codes[i] as usize) < lookup_table[i].len() {
                sum += lookup_table[i][codes[i] as usize];
            }
            i += 1;
        }
        
        sum.sqrt()
    }
}

/// Fallback scalar ADC distance
#[cfg(not(all(target_arch = "x86_64", target_feature = "avx2")))]
pub fn adc_distance_simd(codes: &[u8], lookup_table: &[Vec<f32>]) -> f32 {
    let mut distance = 0.0;
    
    for (subvector_idx, code) in codes.iter().enumerate() {
        if subvector_idx < lookup_table.len() {
            let code_idx = *code as usize;
            if code_idx < lookup_table[subvector_idx].len() {
                distance += lookup_table[subvector_idx][code_idx];
            }
        }
    }
    
    distance.sqrt()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_l2_distance_simd() {
        let a = vec![0.0, 0.0, 0.0, 0.0];
        let b = vec![3.0, 4.0, 0.0, 0.0];
        
        let dist = l2_distance_simd(&a, &b);
        assert!((dist - 5.0).abs() < 0.001);
    }

    #[test]
    fn test_inner_product_simd() {
        let a = vec![1.0, 2.0, 3.0, 4.0];
        let b = vec![2.0, 3.0, 4.0, 5.0];
        
        let prod = inner_product_simd(&a, &b);
        assert!((prod - 40.0).abs() < 0.001); // 1*2 + 2*3 + 3*4 + 4*5 = 40
    }

    #[test]
    fn test_cosine_distance_simd() {
        // Normalized vectors
        let a = vec![1.0, 0.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0, 0.0];
        
        let dist = cosine_distance_simd(&a, &b);
        assert!(dist < 0.001); // Same vector = 0 distance
    }

    #[test]
    fn test_adc_distance_simd() {
        // Test 1: Simple case with known lookup values
        let codes = vec![0, 1, 2, 3];
        let lookup_table = vec![
            vec![1.0, 2.0, 3.0],
            vec![0.5, 1.5, 2.5],
            vec![0.1, 0.2, 0.3],
            vec![0.05, 0.1, 0.15],
        ];
        
        let dist = adc_distance_simd(&codes, &lookup_table);
        // Expected: sqrt(1.0 + 1.5 + 0.1 + 0.05) = sqrt(2.65) â‰ˆ 1.628
        let expected = (1.0f32 + 1.5f32 + 0.1f32 + 0.05f32).sqrt();
        assert!((dist - expected).abs() < 0.05, "ADC distance {} does not match expected {}", dist, expected);
        
        // Test 2: Zero codes
        let codes_zero = vec![0, 0, 0, 0];
        let dist_zero = adc_distance_simd(&codes_zero, &lookup_table);
        let expected_zero = (1.0f32 + 0.5f32 + 0.1f32 + 0.05f32).sqrt();
        assert!((dist_zero - expected_zero).abs() < 0.01);
        
        // Test 3: Max codes
        let codes_max = vec![2, 2, 2, 2];
        let dist_max = adc_distance_simd(&codes_max, &lookup_table);
        let expected_max = (3.0f32 + 2.5f32 + 0.3f32 + 0.15f32).sqrt();
        assert!((dist_max - expected_max).abs() < 0.01);
    }
}
